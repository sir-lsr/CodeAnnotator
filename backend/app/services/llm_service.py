"""
LLMæœåŠ¡ - è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
"""
import json
import requests
from typing import Dict, List, Optional
from openai import OpenAI
import anthropic
from ..config import settings


class LLMService:
    """LLMæœåŠ¡ç±»"""
    
    def __init__(self, provider: str = None, model: str = None, openai_api_key: str = None, openai_base_url: str = None, anthropic_api_key: str = None):
        self.provider = provider or settings.DEFAULT_LLM_PROVIDER
        self.model = model or settings.DEFAULT_MODEL
        self.openai_client = None
        self.anthropic_client = None
        self.ollama_url = "http://localhost:11434"  # Ollama é»˜è®¤åœ°å€
        
        # ä½¿ç”¨ä¼ å…¥çš„ API keyï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„
        final_openai_key = openai_api_key or settings.OPENAI_API_KEY
        final_openai_base_url = openai_base_url or settings.OPENAI_BASE_URL
        final_anthropic_key = anthropic_api_key or settings.ANTHROPIC_API_KEY
        
        if final_openai_key:
            # æ”¯æŒè‡ªå®šä¹‰ OpenAI API åœ°å€
            openai_kwargs = {
                "api_key": final_openai_key,
                "timeout": 180.0  # è®¾ç½® 3 åˆ†é’Ÿè¶…æ—¶ï¼Œé€‚é… DeepSeek ç­‰è¾ƒæ…¢çš„ API
            }
            if final_openai_base_url:
                openai_kwargs["base_url"] = final_openai_base_url
            self.openai_client = OpenAI(**openai_kwargs)
        
        if final_anthropic_key:
            self.anthropic_client = anthropic.Anthropic(api_key=final_anthropic_key)
    
    def generate_line_annotations(self, code: str, language: str) -> Dict:
        """
        ç”Ÿæˆè¡Œå†…æ ‡æ³¨
        
        Args:
            code: ä»£ç å†…å®¹
            language: ç¼–ç¨‹è¯­è¨€
            
        Returns:
            æ ‡æ³¨æ•°æ®å­—å…¸
        """
        prompt = self._build_line_annotation_prompt(code, language)
        
        try:
            # ä½¿ç”¨ Ollama
            if self.provider == "ollama":
                return self._call_ollama(prompt)
            
            # ä½¿ç”¨ OpenAI
            elif self.provider == "openai" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œæ“…é•¿åˆ†æä»£ç å¹¶æä¾›æœ‰ä»·å€¼çš„æ³¨é‡Šã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                
                result = response.choices[0].message.content
                return json.loads(result)
            
            # ä½¿ç”¨ Anthropic
            elif self.provider == "anthropic" and self.anthropic_client:
                message = self.anthropic_client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    temperature=0.3,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                result = message.content[0].text
                return json.loads(result)
            
            else:
                return {"error": f"æœªé…ç½® {self.provider} LLM æˆ– API å¯†é’¥æ— æ•ˆ"}
                
        except Exception as e:
            return self._handle_llm_error(e)
    
    def generate_function_annotations(self, function_code: str, language: str, function_name: str) -> Dict:
        """
        ç”Ÿæˆå‡½æ•°æ ‡æ³¨
        
        Args:
            function_code: å‡½æ•°ä»£ç 
            language: ç¼–ç¨‹è¯­è¨€
            function_name: å‡½æ•°å
            
        Returns:
            æ ‡æ³¨æ•°æ®å­—å…¸
        """
        prompt = self._build_function_annotation_prompt(function_code, language)
        
        try:
            # ä½¿ç”¨ Ollama
            if self.provider == "ollama":
                return self._call_ollama(prompt)
            
            # ä½¿ç”¨ OpenAI
            elif self.provider == "openai" and self.openai_client:
                response = self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç æ–‡æ¡£ç”Ÿæˆä¸“å®¶ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    response_format={"type": "json_object"}
                )
                
                result = response.choices[0].message.content
                return json.loads(result)
            
            # ä½¿ç”¨ Anthropic
            elif self.provider == "anthropic" and self.anthropic_client:
                message = self.anthropic_client.messages.create(
                    model=self.model,
                    max_tokens=2000,
                    temperature=0.3,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                result = message.content[0].text
                return json.loads(result)
            
            else:
                return {"error": f"æœªé…ç½® {self.provider} LLM æˆ– API å¯†é’¥æ— æ•ˆ"}
                
        except Exception as e:
            return self._handle_llm_error(e)
    
    def _call_ollama(self, prompt: str) -> Dict:
        """
        è°ƒç”¨ Ollama API
        
        Args:
            prompt: æç¤ºè¯
            
        Returns:
            æ ‡æ³¨æ•°æ®å­—å…¸
        """
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model or "codellama:7b",
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                    }
                },
                timeout=120  # Ollama å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
            )
            
            if response.status_code == 200:
                result = response.json()
                response_text = result.get("response", "")
                
                # å°è¯•è§£æ JSON
                try:
                    # æå– JSON éƒ¨åˆ†ï¼ˆå¯èƒ½åŒ…å«åœ¨ markdown ä»£ç å—ä¸­ï¼‰
                    if "```json" in response_text:
                        json_str = response_text.split("```json")[1].split("```")[0].strip()
                    elif "```" in response_text:
                        json_str = response_text.split("```")[1].split("```")[0].strip()
                    else:
                        json_str = response_text.strip()
                    
                    return json.loads(json_str)
                except json.JSONDecodeError:
                    # å¦‚æœæ— æ³•è§£æ JSONï¼Œè¿”å›åŸå§‹å“åº”
                    return {
                        "error": "Ollama è¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼",
                        "raw_response": response_text[:500]
                    }
            else:
                return {
                    "error": f"Ollama API è°ƒç”¨å¤±è´¥: HTTP {response.status_code}",
                    "detail": response.text
                }
                
        except requests.exceptions.ConnectionError:
            return {
                "error": "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ ğŸ”Œ",
                "detail": "è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ\nè¿è¡Œå‘½ä»¤: ollama serve",
                "solution": "1. å¯åŠ¨ Ollama æœåŠ¡\n2. æˆ–æ£€æŸ¥ Ollama æ˜¯å¦å·²å®‰è£…"
            }
        except requests.exceptions.Timeout:
            return {
                "error": "Ollama å“åº”è¶…æ—¶ â±ï¸",
                "detail": "æ¨¡å‹å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œè¯·ç¨åé‡è¯•",
                "solution": "1. ä½¿ç”¨æ›´å°çš„ä»£ç ç‰‡æ®µ\n2. æˆ–ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹"
            }
        except Exception as e:
            return {
                "error": "Ollama è°ƒç”¨å¤±è´¥",
                "detail": str(e)
            }
    
    def _handle_llm_error(self, error: Exception) -> Dict:
        """å¤„ç† LLM é”™è¯¯ï¼Œè¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
        error_msg = str(error)
        
        # è¶…æ—¶é”™è¯¯
        if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
            return {
                "error": "API å“åº”è¶…æ—¶ â±ï¸",
                "detail": "AI æ¨¡å‹å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œè¯·å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š",
                "solution": "1. å‡å°‘ä»£ç æ–‡ä»¶å¤§å°ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰\n2. ç¨åé‡è¯•\n3. æ£€æŸ¥ç½‘ç»œè¿æ¥\n4. è€ƒè™‘åˆ‡æ¢åˆ°æ›´å¿«çš„æ¨¡å‹",
                "error_code": "timeout"
            }
        # 429 - ä½™é¢ä¸è¶³
        elif "insufficient_quota" in error_msg or "429" in error_msg:
            return {
                "error": "API è´¦æˆ·ä½™é¢ä¸è¶³ ğŸ’°",
                "detail": "è¯·è®¿é—®æœåŠ¡å•†ç½‘ç«™å……å€¼è´¦æˆ·æˆ–åœ¨è®¾ç½®ä¸­åˆ‡æ¢åˆ°å…è´¹çš„ Ollama æœ¬åœ°æ–¹æ¡ˆ",
                "solution": "1. å……å€¼è´¦æˆ·\n2. æˆ–ä½¿ç”¨å…è´¹çš„ Ollamaï¼ˆæŸ¥çœ‹æ–‡æ¡£ï¼šå¿«é€Ÿä½¿ç”¨Ollama.mdï¼‰",
                "error_code": "insufficient_quota"
            }
        # 401 - å¯†é’¥æ— æ•ˆ
        elif "invalid_api_key" in error_msg or "401" in error_msg or "Unauthorized" in error_msg:
            return {
                "error": "API å¯†é’¥æ— æ•ˆ ğŸ”‘",
                "detail": "è¯·åœ¨è®¾ç½®é¡µé¢æ£€æŸ¥å¹¶æ›´æ–°æ‚¨çš„ API å¯†é’¥",
                "solution": "1. ç¡®è®¤ API å¯†é’¥æ­£ç¡®\n2. ç¡®è®¤ API åœ°å€æ­£ç¡®\n3. æ£€æŸ¥å¯†é’¥æ˜¯å¦è¿‡æœŸ",
                "error_code": "invalid_api_key"
            }
        # 429 - é€Ÿç‡é™åˆ¶
        elif "rate_limit" in error_msg:
            return {
                "error": "API è°ƒç”¨é¢‘ç‡è¿‡å¿« â±ï¸",
                "detail": "è¯·ç¨åå†è¯•ï¼Œæˆ–å‡çº§è´¦æˆ·ä»¥è·å¾—æ›´é«˜é™é¢",
                "error_code": "rate_limit"
            }
        # å…¶ä»–é”™è¯¯
        else:
            return {
                "error": "LLM è°ƒç”¨å¤±è´¥",
                "detail": error_msg,
                "solution": "è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ API é…ç½®"
            }
    
    def _build_line_annotation_prompt(self, code: str, language: str) -> str:
        """æ„å»ºè¡Œå†…æ ‡æ³¨æç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç å®¡æŸ¥ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹{language}ä»£ç æ·»åŠ è¡Œå†…æ³¨é‡Šã€‚

ä»£ç å†…å®¹:
```{language}
{code}
```

è¦æ±‚:
1. ä¸ºé‡è¦çš„ä»£ç è¡Œæ·»åŠ ç®€æ´çš„ä¸­æ–‡æ³¨é‡Šï¼ˆä¸éœ€è¦æ¯è¡Œéƒ½æ ‡æ³¨ï¼‰
2. æ ‡æ³¨ç±»å‹åŒ…æ‹¬:
   - info: åŠŸèƒ½è¯´æ˜å’Œä»£ç è§£é‡Š
   - warning: æ½œåœ¨é—®é¢˜æˆ–éœ€è¦æ³¨æ„çš„åœ°æ–¹
   - suggestion: ä¼˜åŒ–å»ºè®®
   - security: å®‰å…¨ç›¸å…³æç¤º
3. åªæ ‡æ³¨çœŸæ­£é‡è¦çš„è¡Œï¼ˆçº¦10-20%çš„ä»£ç è¡Œï¼‰
4. è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼

è¿”å›æ ¼å¼ç¤ºä¾‹:
{{
  "annotations": [
    {{
      "line": 5,
      "type": "info",
      "content": "åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± "
    }},
    {{
      "line": 12,
      "type": "warning",
      "content": "æœªè¿›è¡Œè¾“å…¥éªŒè¯ï¼Œå¯èƒ½å­˜åœ¨æ³¨å…¥é£é™©"
    }}
  ]
}}

è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"""
    
    def _build_function_annotation_prompt(self, function_code: str, language: str) -> str:
        """æ„å»ºå‡½æ•°æ ‡æ³¨æç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç æ–‡æ¡£ç”Ÿæˆä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹{language}å‡½æ•°ç”Ÿæˆè¯¦ç»†çš„æ–‡æ¡£ã€‚

å‡½æ•°ä»£ç :
```{language}
{function_code}
```

è¦æ±‚:
1. ç”Ÿæˆæ¸…æ™°çš„ä¸­æ–‡å‡½æ•°åŠŸèƒ½æè¿°
2. è¯´æ˜æ¯ä¸ªå‚æ•°çš„åç§°ã€ç±»å‹å’Œç”¨é€”
3. è¯´æ˜è¿”å›å€¼çš„ç±»å‹å’Œå«ä¹‰
4. å¦‚æœå¯èƒ½ï¼Œæä¾›ä¸€ä¸ªç®€å•çš„ä½¿ç”¨ç¤ºä¾‹
5. è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼

è¿”å›æ ¼å¼ç¤ºä¾‹:
{{
  "function_name": "calculate_total",
  "description": "è®¡ç®—è®¢å•æ€»ä»·ï¼ŒåŒ…å«ç¨è´¹å’ŒæŠ˜æ‰£",
  "parameters": [
    {{
      "name": "items",
      "type": "List[Item]",
      "description": "è®¢å•å•†å“åˆ—è¡¨"
    }},
    {{
      "name": "discount",
      "type": "float",
      "description": "æŠ˜æ‰£ç‡ï¼ŒèŒƒå›´0-1"
    }}
  ],
  "returns": {{
    "type": "float",
    "description": "è®¡ç®—åçš„æ€»ä»·"
  }},
  "example": "total = calculate_total(items, 0.1)  # åº”ç”¨10%æŠ˜æ‰£"
}}

è¯·ç›´æ¥è¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"""


# åˆ›å»ºå…¨å±€å®ä¾‹ï¼ˆé»˜è®¤é…ç½®ï¼‰
llm_service = LLMService()

# è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®è®¾ç½®åˆ›å»º LLM æœåŠ¡å®ä¾‹
def get_llm_service(provider: str = None, model: str = None, openai_api_key: str = None, openai_base_url: str = None, anthropic_api_key: str = None) -> LLMService:
    """
    è·å– LLM æœåŠ¡å®ä¾‹
    
    Args:
        provider: LLM æä¾›å•† (openai, anthropic, ollama)
        model: æ¨¡å‹åç§°
        openai_api_key: OpenAI API å¯†é’¥
        openai_base_url: OpenAI API åŸºç¡€ URL (æ”¯æŒ DeepSeek ç­‰å…¼å®¹æœåŠ¡)
        anthropic_api_key: Anthropic API å¯†é’¥
        
    Returns:
        LLMService å®ä¾‹
    """
    return LLMService(provider=provider, model=model, openai_api_key=openai_api_key, openai_base_url=openai_base_url, anthropic_api_key=anthropic_api_key)


